# 详解几种常见本地大模型个人知识库工具部署、微调及对比选型（1）-腾讯云开发者社区-腾讯云
[详解几种常见本地大模型个人知识库工具部署、微调及对比选型（1）-腾讯云开发者社区-腾讯云](https://cloud.tencent.com/developer/article/2426551) 

 这几年，各种新技术、新产品层出不穷，其中，**大模型（Large Language Models）**作为AI领域的颠覆性创新，凭借其在语言生成、理解及多任务适应上的卓越表现，迅速点燃了科技界的热情。从阿尔法狗的胜利到GPT系列的横空出世，大模型不仅展现了[人工智能](https://cloud.tencent.com/product/ai-class?from_column=20065&from=20065)前所未有的创造力与洞察力，也预示着智能化转型的新纪元。然而，大模型的潜力要真正转化为生产力，实现从实验室到现实世界的平稳着陆，还需跨越理论到实践的鸿沟。

而在这一过程中，一个概念应运而生——**大模型知识库**。这一概念的核心在于，它不仅仅是对现有企业私有知识库的技术性升级，更是一种革命性的知识管理与利用方式。大模型知识库旨在通过融合最新的大模型技术，对企业的海量内部数据、专业知识、最佳实践等进行高效组织、智能索引和深度学习，使之成为可被模型理解和运用的结构化知识资源。

这样的知识库不仅能够实现对企业内部知识的快速检索和精准匹配，还能够借助大模型的语境理解和生成能力，自动总结文档、生成报告、解答复杂问题，甚至在特定领域内进行创新性思考和策略建议。换句话说，**大模型知识库可以成为企业智慧的“超级大脑”，极大提升知识的流动性和价值转化效率，让企业的每一份知识资产都成为推动业务发展和创新的源泉。** 

同理，既然企业可以用大模型知识库来管理企业级的知识，那么个人同样也可以构建起**个人版的“智慧大脑”**。想象一下，将个人的学习笔记、工作经验、技能树、甚至是兴趣爱好等各类信息，全部整合进这样一个智能化的知识管理体系中。这不仅是一个简单的信息存储仓库，而是一个能够**自我学习**、**自我优化**，并**根据个人需求动态调整**的知识生态系统。

所以，这篇文章，我们就来好好聊一下最近一段时间常见的本地大模型个人知识库工具。至于为什么聊这个话题呢？有两个原因。

一是因为之前其实已经有过相关涉猎了，如之前有尝试过基于Ollama+AnythingLLM轻松打造本地大模型知识库，这篇文章放在整个互联网上同类型里面也算是比较早发表的，可惜事后尝试总觉得效果不如人意，缺乏自定义能力，因此想多研究几个开源工具，进行对比选型，找出更符合自己要求的。

二是因为最近同事也拜托我给她的新电脑搭建了一套本地大模型知识库环境，这次采用的是MaxKB来实现的，由于是纯windows环境部署，一路上也是遇到了不少坑，这里也正好想复盘一下。

常见本地大模型知识库工具
------------

这里还是先盘点一下最近比较火爆的几个工具吧，下面分为**知识库侧**和**大模型侧**两个方面来说。

### 知识库侧

知识库侧主要是指更加偏向于能够直接读取文档并处理大量信息资源，包括文档上传、自动抓取在线文档，然后进行文本的自动分割、向量化处理，以及实现本地检索增强生成（RAG）等功能的工具，近期较为热门的主要包括：**AnythingLLM**、**MaxKB**、**RAGFlow**、**FastGPT**、**Dify** 、 **Open WebUI** 这六种。

#### AnythingLLM

这个也就是我之前使用过但是觉得效果不太理想的那位，稍微简单的介绍一下吧。

AnythingLLM 是 Mintplex Labs Inc. 开发的一款可以与任何内容聊天的私人 ChatGPT，是高效、可定制、开源的企业级文档聊天机器人解决方案。它能够将任何文档、资源或内容片段转化为大语言模型（LLM）在聊天中可以利用的相关上下文。

其采用MIT许可证的开源框架，支持快速在本地部署基于检索增强生成（RAG）的大模型应用。在不调用外部接口、不发送本地数据的情况下，确保用户数据的安全。

最近 AnythingLLM推出了桌面应用，可以在自己的笔记本电脑上下载使用，目前支持的操作系统包括MacOS，Windows和Linux。

这个也就是我之前使用过但是觉得效果不太理想的那位，稍微简单的介绍一下吧。

AnythingLLM 是 Mintplex Labs Inc. 开发的一款可以与任何内容聊天的私人 ChatGPT，是高效、可定制、开源的企业级文档聊天机器人解决方案。它能够将任何文档、资源或内容片段转化为大语言模型（LLM）在聊天中可以利用的相关上下文。

其采用MIT许可证的开源框架，支持快速在本地部署基于检索增强生成（RAG）的大模型应用。在不调用外部接口、不发送本地数据的情况下，确保用户数据的安全。

最近 AnythingLLM推出了桌面应用，可以在自己的笔记本电脑上下载使用，目前支持的操作系统包括MacOS，Windows和Linux。

![](https://developer.qcloudimg.com/http-save/yehe-9667716/901b64828c91f21246cbe49f613b8f39.png)

值得一提的是，AnythingLLM可以支持PDF，TXT，DOCX等文档，可以提取文档中的文本信息，经过嵌入模型（Embedding Models），保存在[向量数据库](https://cloud.tencent.com/product/vdb?from_column=20065&from=20065)中，并通过一个简单的UI界面管理这些文档。

为管理这些文档，AnythingLLM引入**工作区（workspace）**的概念，作为文档的容器，可以在一个工作区内共享文档，但是工作区之间隔离。

同时，它独特的多用户模式，配合工作区使用起来效果更佳：

*   管理员（Admin）账号：拥有全部的管理权限。
*   Manager账号：可管理所有工作区和文档，但是不能管理大模型、嵌入模型和向量数据库。
*   普通用户账号：基于已授权的工作区与大模型对话，不能对工作区和系统配置做任何更改。

#### MaxKB

MaxKB是一款基于 LLM 大语言模型的知识库问答系统。MaxKB = Max Knowledge Base，旨在成为企业的最强大脑。

![](https://developer.qcloudimg.com/http-save/yehe-9667716/dc5df33e410c0a816e39a0c7996a467e.png)

与同类基于LLM的知识库问答提供系统相比，MaxKB的核心优势包括：

■ 开箱即用：支持直接上传文档、自动爬取在线文档，支持文本自动拆分、向量化，智能问答交互体验好；

■ 无缝嵌入：支持零编码快速嵌入到第三方业务系统；

■ 多模型支持：支持对接主流的大模型，包括本地私有大模型（如Llama 2）、OpenAI、Azure OpenAI和百度千帆大模型等。

使用界面是这个样子：

![](https://developer.qcloudimg.com/http-save/yehe-9667716/04ee14832f2caed51baee2eb5cad49d5.png)

*   官方推荐采用**docker**进行快速部署，所以我个人建议还是起一台Linux虚拟机，方便又快捷；
*   如果稍微懂一点Linux，但是不太懂docker的，建议部署 1Panel 后再通过里面的应用商店来下载使用；
*   如果实在不会Linux的才建议安装**Docker Desktop**，但是安装的过程会麻烦很多，包括开启Hyper-v、CPU虚拟化、启用WSL、修改环境变量为非家庭版等（Docker Desktop不支持家庭版哦！），一套下来顺利的话也得至少半个小时以上。

上面这三种部署方式，后续也都会详细讲解到。

#### RAGFlow

RAGFlow 作为一款端到端的RAG解决方案，旨在通过深度文档理解技术，解决现有RAG技术在数据处理和生成答案方面的挑战。它不仅能够处理多种格式的文档，还能够智能地识别文档中的结构和内容，从而确保数据的高质量输入。RAGFlow 的设计哲学是“高质量输入，高质量输出”，它通过提供可解释性和可控性的生成结果，让用户能够信任并依赖于系统提供的答案。

2024年4月1日，RAGFlow宣布正式开源，这一消息在技术界引起了轰动。开源当天，RAGFlow 在 GitHub 上迅速获得了数千的关注，不到一周时间，已吸收2900颗星，这不仅体现了社区对 RAGFlow 的高度认可，也显示出大家对这一新技术的热情。

![](https://developer.qcloudimg.com/http-save/yehe-9667716/7223e9040162efae61c402d6747d8c4f.png)

*   **深度文档理解**："Quality in, quality out"，RAGFlow 基于深度文档理解，能够从各类复杂格式的非结构化数据中提取真知灼见。真正在无限上下文（token）的场景下快速完成大海捞针测试。对于用户上传的文档，它需要自动识别文档的布局，包括标题、段落、换行等，还包含难度很大的图片和表格。对于表格来说，不仅仅要识别出文档中存在表格，还会针对表格的布局做进一步识别，包括内部每一个单元格，多行文字是否需要合并成一个单元格等。并且表格的内容还会结合表头信息处理，确保以合适的形式送到数据库，从而完成 RAG 针对这些细节数字的“大海捞针”。
*   **可控可解释的文本切片**：RAGFlow 提供多种文本模板，用户可以根据需求选择合适的模板，确保结果的可控性和可解释性。因此 RAGFlow 在处理文档时，给了不少的选择：Q&A，Resume，Paper，Manual，Table，Book，Law，通用... 。当然，这些分类还在不断继续扩展中，处理过程还有待完善。后续还会抽象出更多共通的东西，使各种定制化的处理更加容易。
*   **降低幻觉**：RAGFlow 是一个完整的 RAG 系统，而目前开源的 RAG，大都忽视了 RAG 本身的最大优势之一：可以让 LLM 以可控的方式回答问题，或者换种说法：有理有据、消除幻觉。我们都知道，随着模型能力的不同，LLM 多少都会有概率会出现幻觉，在这种情况下， 一款 RAG 产品应该随时随地给用户以参考，让用户随时查看 LLM 是基于哪些原文来生成答案的，这需要同时生成原文的引用链接，并允许用户的鼠标 hover 上去即可调出原文的内容，甚至包含图表。如果还不能确定，再点一下便能定位到原文。RAGFlow 的文本切片过程可视化，支持手动调整，答案提供关键引用的快照并支持追根溯源，从而降低幻觉的风险。
*   **兼容各类异构数据源**：RAGFlow 支持 支持丰富的文件类型，包括 Word 文档、PPT、excel 表格、txt 文件、图片、PDF、影印件、复印件、结构化数据, 网页等。对于无序文本数据，RAGFlow 可以自动提取其中的关键信息并转化为结构化表示；而对于结构化数据，它则能灵活切入，挖掘内在的语义联系。最终将这两种不同来源的数据统一进行索引和检索，为用户提供一站式的数据处理和问答体验。
*   **自动化 RAG 工作流**：RAGFlow 支持全面优化的 RAG 工作流可以支持从个人应用乃至超大型企业的各类生态系统；大语言模型 LLM 以及向量模型均支持配置，用户可以根据实际需求自主选择。；基于多路召回、融合重排序，能够权衡上下文语义和关键词匹配两个维度，实现高效的相关性计算；提供易用的 API，可以轻松集成到各类企业系统，无论是对个人用户还是企业开发者，都极大方便了二次开发和系统集成工作。

#### FastGPT

FastGPT 是一个基于 LLM 大语言模型的知识库问答系统，提供开箱即用的数据处理、模型调用等能力。同时可以通过 Flow 可视化进行工作流编排，从而实现复杂的问答场景！

![](https://developer.qcloudimg.com/http-save/yehe-9667716/96f74ead1fc32cd3532818da296190a5.png)

该项目主要提供了以下几个核心特点和功能：

*   开箱即用的数据处理与模型调用：FastGPT允许用户轻松地导入文档、文本文档、PDF文件、电子邮件等非结构化数据，并自动处理这些数据以便于模型理解和使用。它内置了对多种大型语言模型的支持，如GPT-2、GPT-3及其变体，用户可以快速调用这些模型进行问答或对话任务。
*   可视化工作流编排：FastGPT支持使用Flow可视化工具进行工作流的编排，使得构建复杂问答场景变得更加直观和简单，用户可以拖拽组件来设计数据处理流程、模型调用逻辑等，而不需要编写大量代码。
*   高效向量检索：项目利用PostgreSQL的PG Vector插件作为向量检索器，优化了对大规模数据的检索效率，这在处理复杂知识库查询时尤为关键。
*   易于部署与定制：FastGPT设计有较低的学习成本，便于用户快速上手和部署。同时，它支持模型的微调和扩展，用户可以根据特定需求调整模型，使其更好地服务于特定领域或业务场景。

#### Dify

Dify是一款开源的大语言模型(LLM) 应用开发平台。它融合了后端即服务（Backend as Service）和 LLMOps 的理念，使开发者可以快速搭建生产级的生成式 AI 应用。即使你是非技术人员，也能参与到 AI 应用的定义和数据运营过程中。

由于 Dify 内置了构建 LLM 应用所需的关键技术栈，包括对数百个模型的支持、直观的 Prompt 编排界面、高质量的 RAG 引擎、稳健的 Agent 框架、灵活的流程编排，并同时提供了一套易用的界面和 API。这为开发者节省了许多重复造轮子的时间，使其可以专注在创新和业务需求上。

![](https://developer.qcloudimg.com/http-save/yehe-9667716/902a5cc6512516480524780362055c56.png)

你或许可以把 LangChain 这类的开发库（Library）想象为有着锤子、钉子的工具箱。与之相比，Dify 提供了更接近生产需要的完整方案，Dify 好比是一套脚手架，并且经过了精良的工程设计和软件测试。

重要的是，Dify 是开源的，它由一个专业的全职团队和社区共同打造。你可以基于任何模型自部署类似 Assistants API 和 GPTs 的能力，在灵活和安全的基础上，同时保持对数据的完全控制。

#### Open WebUI

Open WebUI（前身为Ollama WebUI）是一个可扩展的、功能丰富的、用户友好的自托管Web界面，设计用于完全离线运行。它支持各种LLM（大型语言模型）运行器，包括Ollama和兼容OpenAI的API。

特性包括：

*   直观界面：聊天界面灵感源自 ChatGPT，确保用户友好体验。
*   响应式设计：在桌面和移动设备上都能获得无缝体验。
*   **本地RAG集成**：通过开创性的检索增强生成（RAG）支持，深入聊天互动的未来。这项功能将文档交互无缝集成到你的聊天体验中。你可以直接将文档加载到聊天中，或者毫不费力地将文件添加到你的文档库中，使用提示中的#命令轻松访问它们。在其alpha阶段，可能会出现偶尔的问题，因为我们正在积极改进和增强这项功能，以确保最佳性能和可靠性。
*   无缝设置：使用 Docker 或 Kubernetes（kubectl、kustomize 或 helm）轻松安装，无烦恼体验。
*   主题定制、代码语法高亮、完整支持 Markdown 和 LaTeX、本地 RAG 整合、RAG 嵌入支持、网页浏览功能。

该项目还具有诸多功能，支持多模型和多语言设置，旨在提供全面的聊天体验，提高用户互动的灵活性和多样性。

![](https://developer.qcloudimg.com/http-save/yehe-9667716/0fbd46e8c4238f9fd85b2127cdac4419.png)

#### 小结

纵观上述知识库侧的六种工具，我们不难发现其共同点：都强调了对**检索增强生成**（**RAG**, Retrieval Enhanced Generation）的支持。RAG是一种结合了检索和生成两种策略的技术，旨在提升模型的性能，尤其是在处理需要精确信息检索和上下文理解的任务上。而RAG的准确性，则决定了本地知识库最终生成答案的质量与实用性，工具能否**支持用户实现**或者**让用户能以更小的代价、更简单的方式**实现RAG，是评判知识库侧工具能力的关键点。

其次需要考虑的点就是这些工具能否满足**多样化的模型集成**与**高度的可定制性**的要求。要既能对接外部模型比如：通义千问、OpenAI、Azure OpenAI等，也能对接本地**大模型侧工具**如Ollama，确保了广泛的应用覆盖和适应性。

再者，需要考虑的才是**用户体验与界面**等方面。综合以上几点，因此，方能挑选出最为合适的知识库侧工具。

### 大模型侧

大模型侧理论上是需要对模型本身进行测评的，但是本人确实无此资质，所以在此不做对于任何模型的评测。如有需求，可以直接查看CompassArena 司南大模型竞技场给出的排行榜，并根据实际情况挑选适合自己的模型。

这里主要讨论用来管理或者快捷部署本地大模型的工具，较为热门的主要包括：**Ollama**、**LM Studio**、**Xinference**等。

#### Ollama

最近被刷爆的**唯一真神**！

Ollama是一个开源的大型语言模型服务工具，它帮助用户快速在本地运行大模型。通过简单的安装指令，用户可以执行一条命令就在本地运行开源大型语言模型，如Llama 2和最新开源的Llama 3。Ollama极大地简化了在Docker容器内部署和管理LLM的过程，使得用户能够快速地在本地运行大型语言模型。

![](https://developer.qcloudimg.com/http-save/yehe-9667716/626f10a20f8301f4df7b8fab1949afe0.png)

在6月2日，Ollama也推出了他的 **0.1.40 - 0.1.41** 版本，新推出了三个模型：

*   Codestral：这是 Mistral AI 首次推出的代码生成模型，专门为编码任务设计。
*   IBM Granite Code：专门为编码任务设计，目前提供 3B 和 8B 两种模型大小。
*   Deepseek V2：一款强大、经济又高效的混合专家语言模型。

目前为止，Ollama几乎可以说是大模型工具侧的神，极其推荐使用！

#### LM Studio

LM Studio，这款丝毫**不逊色于Ollama**！

LM Studio是一款功能强大、易于使用的桌面应用程序，用于在本地机器上实验和评估大型语言模型（LLMs）。它允许用户轻松地比较不同的模型，并支持使用 NVIDIA/AMD GPU 加速计算。

使用LM Studio不需要深厚的技术背景或复杂的安装过程。它提供了一个简单的安装程序，用户只需几个简单的步骤就可以轻松安装和运行。

![](https://developer.qcloudimg.com/http-save/yehe-9667716/8d36d61641c79afe044cc3e182e2f094.png)

#### Xinference

相比于上面两位重量级的，这位最近在互联网上就稍显冷门了。

Xorbits Inference是一个性能强大且功能全面的分布式推理框架。可用于各种模型的推理。通过 Xinference，你可以轻松地一键部署你自己的模型或内置的前沿开源模型。无论你是研究者，开发者，或是数据科学家，都可以通过 Xinference 与最前沿的 AI 模型，发掘更多可能。

![](https://developer.qcloudimg.com/http-save/yehe-9667716/adf7d8c58b43518dbf5e6505b4e0cded.png)

可能在界面上没有上面二位那么美观，但是基础功能还是比较齐全的，提供了简洁的API来集成模型到应用，还便于模型管理和高性能的基础设施，保证了在复杂模型运行的效率。

#### 小结

纵观上述大模型侧的三种工具，我们也可以看出，能**高效部署和快速使用**是大模型侧工具选择的第一要义，其次是工具的**定制与可扩展性**，再其次是**易用性、稳定性**。这四个维度综合起来，为大模型工具选择奠定了坚实的基础，使得项目能够既高效推进，又能适应变化，稳定可靠，同时满足定制需求，操作友好。因此，评估时这四点缺一不可，方能挑选出最为合适的大模型侧工具。

未完待续>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

感谢[腾讯云开发者社区](https://cloud.tencent.com/developer/?from_column=20065&from=20065)小伙伴的陪伴，如果你喜欢我的博客内容，认可我的观点和经验分享，请点赞、收藏和评论，这将是对我最大的鼓励和支持。同时，也欢迎大家提出宝贵的意见和建议，让我能够更好地改进和完善我的博客。谢谢！[我正在参与2024腾讯技术创作特训营最新征文，快来和我瓜分大奖！](https://cloud.tencent.com/developer/article/2423305?from_column=20421&from=20421)